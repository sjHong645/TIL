CPU 스케쥴러의 평가 기준은 다음과 같다.
| 기준 | 설명 |
| --- | --- |
| CPU utilization | CPU를 얼마나 지속적으로 활용하는지 | 
| Through-put | 일정 시간 단위 동안 실행을 완료한 프로세스의 개수(요즘에는 이 기준은 잘 안 따짐) | 
| Turn-around time | 프로세스를 수행하는데 걸린 총 시간 | 
| Waiting time | turn-around time에 속해 있는 시간으로 waiting queue에서 기다리는 시간 | 
| Response time | request를 받고 첫 response를 만들 때까지 걸린 시간 | 

이러한 기준을 바탕으로 최상의 스케쥴링 알고리즘을 만든다고 한다면 CPU 활용률과 throughput은 최대가 되면 좋고  
Turn-around time, Waiting time, Response time은 최소가 되면 좋다.

하지만, 이런 기준을 모두 만족한다는 건 불가능하기 때문에 내가 작업하고자 하는 시스템에 따라서 어떤 요소에 focus를 맞출지 결정한다.

ex. Super computer 는... cpu 활용률을 최대화하는 것에 목적 / 개인용 컴퓨터는 응답시간을 최소한으로 하는 것에 목적

스케쥴링 알고리즘으로는 아래와 같이 여러가지 방법이 있다.
- First-Come, First-Served Scheduling
- Shortest-Job First Scheduling
- Priority Scheduling
- Round Robin Scheduling
- Multilevel Queue Scheduling

 각각에 대해서 살펴보자.
 
 ### First-Come, First-Served (FCFS) Scheduling : 선입, 선처리 스케쥴링

![image](https://user-images.githubusercontent.com/64796257/147730498-1b12cda7-d25d-4030-80cf-844e8814ebb5.png)

프로세스들이 P1, P2, P3 순으로 도착하고 선입 선처리 순으로 서비스를 받을 때 위와 같은 Gantt Chart를 그릴 수 있다. 

`P1`은 도착하자마자 바로 실행했기 때문에 `대기 시간 = 0` 이다.  

`P2`는 도착을 했지만 `P1`이 이미 실행중이여서 `P1`이 다 실행되고 나서 실행되기 때문에 24가 지나야 실행된다.  
따라서 `P2의 대기시간은 24`

`P3`는 `P1`과 `P2`의 실행이 모두 끝나야 실행되므로 `P1의 실행시간`과 `P2의 실행시간`을 모두 더한 `24+3 = 27 만큼의 대기시간`이 필요하다.

정리) 대기시간 P1 = 0 / P2 = 24 / P3 = 27 이므로 평균 대기시간은 17이 된다.

그런데, 프로세스가 도착한 순서를 바꾼다면 어떻게 될까?? P2, P3, P1 순으로 바꿨다고 하자.

![image](https://user-images.githubusercontent.com/64796257/147730689-1fade873-5d17-417c-885d-3ec45475d3d4.png)


`P2`는 도착하자마자 바로 실행하기 때문에 `대기 시간이 0`

`P3`는 `P2`가 다 끝나고 나서 실행되기 때문에 `3` 

`P1`은 `P3`와 `P2`가 다 끝나고 나서 실행되기 때문에 `6`

정리) 대기시간 P1 = 6 / P2 = 0 / P3 = 3 이므로 평균 대기시간은 3이 된다. 앞선 상황보다 대기 시간이 훨씬 줄었다.

이처럼 프로세스의 burst time이 가장 긴 프로세스를 나중에 처리하도록 하고 

burst time이 짧은 프로세스를 먼저 수행하도록 해서 전체적인 waiting time을 줄일 수 있었다.

이러한 idea를 통해서 고안된 방법이 SJF 스케쥴링이다.

### Shortest Job First(SJF) Scheduling 

: 각 프로세스의 burst 길이를 계산해서 burst 길이가 작은 순서대로 CPU를 할당한다.  
: 하지만, 이를 실제로 구현하기 어렵다. 각 프로세스의 burst time을 정확하게 알 수 없기 때문이다.  
: 그래서 `예측`을 해야 하는데 

  예를 들어 어떤 프로세스의 이전 burst time이 6이었다면, 다음번에도 6만큼의 burst time이 소요될 것이라고 예측한다.

![image](https://user-images.githubusercontent.com/64796257/147730909-dbda99fd-f477-402d-bbfe-fcaf8e3e73bc.png)

버스트 길이가 작은 순으로 P4->P1->P3->P2 순서로 진행된다. 

각 프로세스의 대기시간을 보면 

`P4`는 도착하자마자 시작하므로 `0` / `P1`은 `P4`가 다 끝나고 나서 실행하니까 `3`  
`P3`은 `P4`와 `P1`이 다 실행되고 나서 시작하니까 `3+6 = 9`  
`P2`는 `P4`와 `P1`과 `P3`가 다 실행되고 나서 실행하니까 `3+6+7 = 16`

정리) P1은 3 / P2는 16 / P3은 9 / P4는 0 ==> 따라서 평균 대기 시간은 7이 되겠다.

* SJF 알고리즘의 발전된 형태는 아래와 같다.

![image](https://user-images.githubusercontent.com/64796257/147730983-1403cffe-b1d3-47fa-856f-d34d0cc7ac1d.png)

각각의 프로세스들은 도착시간이 다 다르고 preemption(선점형) 알고리즘이라 하자.

일단 맨 처음 도착은 `P1`이 했다. 그러다가 1초 뒤에 `P2`가 도착했다. 

이때 `P1`과 `P2`를 비교하는데... `P1`은 1초를 이미 실행해서 실행시간(burst time)이 7초 남았고 `P2`는 4초 남은 상황이다.

따라서, 기존에 실행하는 P1을 멈추고 P2를 실행한다.

`P2`를 실행하고 `P3`가 1초 뒤에 도착했다. 

`P2`는 1초를 이미 실행해서 3초가 남았고 `P3`는 9초가 남았기 때문에 더 적은 시간이 남은 `P2`를 계속 실행한다.

1초 뒤에 `P4`가 도착했다.

`P2`는 2초를 이미 실행해서 2초가 남았고 `P4`는 5초가 남았기 때문에 더 적은 시간이 남은 `P2`를 계속 실행한다.

이 다음부터는 앞서 다뤘던 SJF 스케쥴링을 토대로 실행시간이 가장 적게 남은 순으로 프로세스들을 실행한다.

그렇다면 각 프로세스의 대기 시간은 얼마나 남았을까??

`P1`이 실행된 시간은 10초다. 하지만, 앞서 1초 실행되고 나서 P2와 P4가 실행되는 시간 9초 동안 기다렸으므로 `P1`의 대기 시간은 `9초`다.

`P2`는 1초에 도착하자마자 바로 실행했으므로 `0초`다.

`P3`는 2초에 도착하고 나서 P2, P4, P1이 다 끝날 때까지 기다렸다. 
따라서, 실질적으로 `P3가 기다린 시간`은 `(P2+P4+P5의 실행시간) - 2= 15초`가 되겠다.

`P4`는 3초에 도착하고 나서 P2가 끝날 때까지 기다렸다.
따라서, 실질적으로 P4가 기다린 시간은 `(P2의 실행시간) - 3 = 2`가 되겠다.

정리) P1의 대기시간 9 / P2의 대기시간 0 / P3의 대기시간 15 / P4의 대기시간 2 

### Priority Scheduling 
: burst time에 상관없이 우선순위(Priority)에 따라서 스케쥴링하는 방법  
: 우선순위는 사용자가 정하기 때문에 사용자의 의도대로 스케쥴링 하는 방법이라 할 수 있다.  
쉬운 이해를 위해 아래의 예시를 보자.

![image](https://user-images.githubusercontent.com/64796257/147731351-4fd015e7-1299-4aae-93cb-bed802568b13.png)

말 그대로 우선순위가 높은 프로세스 먼저 수행하도록 하는 스케쥴링 방식이다. 여기서는 값이 작을수록 우선순위가 높다고 했다.

하지만, 우선순위 스케쥴링의 문제점은 `Starvation`이다. ⇒ 우선순위가 낮은 프로세스가 실행하지 않을 수 있다.

만약에 위의 그림과 같이 프로세스의 우선순위를 정했다고 할 때,

`P3`는 `4라는 우선순위`를 가지고 있다.  
그 앞에서 `P1`이 수행하고 있는데 `P1`이 수행되고 있는 와중에 P3보다 높은 우선순위를 가진 새로운 프로세스 `P6`가 도착했다고 하자. 

그렇다면, CPU는 기존에 실행하던 `P1`은 잠시 미루고 더 높은 우선순위를 가진 새로운 프로세스 `P6`를 수행한다.  

그런데... 이러한 과정이 반복된다면... P3는 영원히 실행을 못할 수도 있다.

이를 해결하기 위한 방법 중 하나로 `Aging`이라는 방법이 있다. 

일정 시간이 지나면 실행하지 않은 프로세스의 우선순위를 점차적으로 증가시키면서 starvation을 방지하도록 하는 방법이다. 

### Round Robin(RR)

: 각각의 프로세스는 한 round마다 time slice를 부여받는다. 그렇게 할당받은 시간이 끝나면 해당 프로세스는 preempted가 되고 ready_q에 더해진다. 

ex. A, B, C라는 프로세스는 한 round마다 각각 4ms, 2ms, 3ms의 시간을 부여받는다고 하자.

이때, Round라는 개념을 알아야 하는데 3개의 프로세스가 다 동작하고 나면 9ms 만큼의 시간이 지난다.

이와 같이 전체 프로세스가 동작하는 하나의 단위를 `round`라고 한다. (round의 길이는 유동적)

1 round에서 A, B, C를 스케쥴링하고 다 동작하고 나면 

2 round에서 A, B, C는 기존에 부여받았었던 4ms, 2ms, 3ms를 다시 받는다.  
⇒ 이러한 round가 반복되는 형태로 스케쥴링하는 방식을 `Round Robin 방식`이라 한다.

이때, 한 round에서 각각의 프로세스들이 동작하는 방식은 앞서 배운 여러가지 방식 중 하나를 선택해서 스케쥴링해주면 되겠다.  
(각각의 round에서 어떤 알고리즘을 쓰는지 꼭 명시할 것)

- Time Quantum : 시간을 할당하는 가장 작은 단위 ex. 1ms

⇒ 1ms를 time quantum으로 잡았다면 1ms 마다 프로세스에 timer interrupt를 보낸다.

timer interrupt를 보냈기 때문에 현재 동작 중인 프로세스가 자신의 time slice를 얼마나 사용했는지 확인한다. 

만약에 다 썼다면, 해당 프로세스를 내리고 다른 프로세스를 preempted 한다.

- Time slice : 하나의 round에서 프로세스가 할당받은 시간. ex. TS = 10ms = 10 of time quantum

⇒ 예시를 통해서 설명하면, 컴퓨터 시스템의 time quantum이 1ms인 시스템에서 슬라이스마다 10개의 타임 퀀텀을 부여했다면... 타임 슬라이스는 10 * 1ms = 10ms가 된다. 

만약에 0.5 단위에 어떤 control을 하고 싶어도 할 수 없다. 

왜냐하면, 타임 퀀텀은 가장 작은 시간 단위이기 때문에 더 이상 그 단위를 쪼갤 수 없기 때문이다.

그렇다면... time slice를 어떻게 부여해줘야 할까?? 프로세스 P1, P2의 burst time이 각각 30ms, 10ms라 하자.

1. 각 프로세스의 타임 슬라이스(time slice)를 3ms, 1ms라 했다. 
 
⇒ 10개의 round를 지나야 P1, P2가 끝난다. ⇒ round가 바뀔 때마다 context switching이 빈번하게 발생한다.

⇒ 그만큼 responsiveness가 좋아진다. 

cf) responsiveness

![image](https://user-images.githubusercontent.com/64796257/147732393-3d7ea16a-ef70-4179-9e8d-b428723d6504.png)

P2를 LoL이 동작하는 프로세스라고 하자. 이때, P1이 동작하는 중간에 마우스를 클릭했다. 

1)의 경우 : P1의 타임 슬라이스는 20ms / P2의 타임 슬라이스는 10ms라고 함  
2)의 경우 : P1의 타임 슬라이스는 2ms / P2의 타임 슬라이스는 1ms라고 함 

1)의 경우는 마우스 클릭을 하고 나서 P2에 도달 할 때까지 평균적으로 10ms만큼 걸린다.

2)의 경우는 마우스 클릭을 하고 나서 P2에 도달 할 때까지 평균적으로 1ms 만큼 걸린다. 

즉, 마우스 클릭에 반응하는 속도는 2)의 경우가 1)의 경우보다 10배는 빠르다고 할 수 있다. 이러한 반응 속도를 responsiveness라고 한다.

2. 각 프로세스의 타임 슬라이스를 30ms, 10ms라 했다. ⇒ 이러면 하나의 round에서 모든 프로세스가 끝나기 때문에 FCFS를 쓴 것과 똑같아진다.

⇒ 그래서 적절한 지점을 찾아야 한다.

ex. P1, P2, P3 프로세스에 타임 슬라이스 4를 할당했다.

![image](https://user-images.githubusercontent.com/64796257/147732731-f22fd6c5-1f1b-4777-8824-d92a83fe7e33.png)

P1이 먼저 도착해서 CPU를 실행하는데 4초가 지나서 그 다음 도착한 P2에게 CPU를 할당해주었다.

그래서 P2를 실행하는데 P2의 실행시간은 타임 슬라이스보다 작으므로 4초보다 적은 3초에 P2를 끝내고 다음 프로세스인 P3를 실행했다. 

P3 역시 타임 슬라이스보다 실행시간이 작으므로 3초가 지난 다음에 P1에게 CPU를 넘겼다. 

그 이후에는 P2, P3는 이미 다 실행했기 때문에 P1만 실행하면 되서 위와 같은 Gantt chart가 나오게 된다.

### Multilevel Queue Scheduling (다단계 큐 스케쥴링) 

ready_q를 두 개의 부분 foreground와 background로 나눈다고 하자.

이때, foreground는 Round Robin 방식으로 동작하고 background는 FCFS 방식으로 동작한다.

여기서 foreground는 80%의 비율로 background는 20%의 비율로 동작한다고 할 때 다음과 같이 그림을 그릴 수 있다. 

![image](https://user-images.githubusercontent.com/64796257/147732831-2cdc32fd-d5ce-47ee-9c05-8fb973c6961e.png)

이와 같이 하나의 시스템에서 여러 개의 프로세스가 서로 다른 방식으로 동작하도록 하는 스케쥴링을 multilevel 스케쥴링이라 한다.

ex. `80이라는 시간 동안은 Round Robin 방식`을 통해 responsiveness를 높이도록 하고 

해당 프로세스의 동작이 끝나면 `20이라는 시간 동안은 FCFS 방식`을 통해서 CPU의 활용성을 높이도록 했다.

장점: 하나의 시스템에서 다양한 특성을 가진 프로세스들을 동작할 수 있다.

단점: 처음에 프로세스를 동작할 때 그 프로세스가 어떤 특성을 가진 프로세스인지 모르기 때문에 어떻게 처리해야 할 지 알 수 없다.










